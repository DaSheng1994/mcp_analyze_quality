#!/usr/bin/env python3

import urllib.request
import urllib.error
import re
import csv
import io
from typing import Dict, Any, List
from datetime import datetime

# è¿™ä¸ªç‰ˆæœ¬éœ€è¦å®‰è£…: pip install mcp
try:
    from mcp.server.fastmcp import FastMCP
    MCP_AVAILABLE = True
except ImportError:
    MCP_AVAILABLE = False
    print("Warning: MCP library not installed. Run 'pip install mcp' to use FastMCP version.")

# å‘Šè­¦é˜ˆå€¼é…ç½®
ALERT_THRESHOLDS = {
    "memory_leak": {
        "growth_rate_critical": 100,  # å†…å­˜å¢é•¿ç‡è¶…è¿‡100%ä¸ºä¸¥é‡
        "growth_rate_warning": 50,    # å†…å­˜å¢é•¿ç‡è¶…è¿‡50%ä¸ºè­¦å‘Š
        "continuous_growth_periods": 5,  # è¿ç»­å¢é•¿å‘¨æœŸæ•°
        "peak_to_avg_ratio": 1.5,    # å³°å€¼ä¸å¹³å‡å€¼æ¯”ä¾‹
    },
    "thread_management": {
        "max_threads_critical": 400,  # çº¿ç¨‹æ•°è¶…è¿‡400ä¸ºä¸¥é‡
        "max_threads_warning": 350,   # çº¿ç¨‹æ•°è¶…è¿‡350ä¸ºè­¦å‘Š
        "thread_variance_critical": 100,  # çº¿ç¨‹æ³¢åŠ¨è¶…è¿‡100ä¸ºä¸¥é‡
        "thread_variance_warning": 50,    # çº¿ç¨‹æ³¢åŠ¨è¶…è¿‡50ä¸ºè­¦å‘Š
    },
    "heap_usage": {
        "heap_growth_critical": 200,  # å †å¢é•¿ç‡è¶…è¿‡200%ä¸ºä¸¥é‡
        "heap_growth_warning": 100,   # å †å¢é•¿ç‡è¶…è¿‡100%ä¸ºè­¦å‘Š
        "heap_utilization_critical": 0.9,  # å †åˆ©ç”¨ç‡è¶…è¿‡90%ä¸ºä¸¥é‡
    },
    "system_resources": {
        "fd_critical": 800,           # æ–‡ä»¶æè¿°ç¬¦è¶…è¿‡800ä¸ºä¸¥é‡
        "fd_warning": 600,            # æ–‡ä»¶æè¿°ç¬¦è¶…è¿‡600ä¸ºè­¦å‘Š
        "views_growth_critical": 200, # è§†å›¾å¢é•¿ç‡è¶…è¿‡200%ä¸ºä¸¥é‡
    }
}

def analyze_memory_leak_risk(data_rows: List[Dict], stats: Dict) -> Dict[str, Any]:
    """åˆ†æå†…å­˜æ³„æ¼é£é™©"""
    alerts = {
        "risk_level": "ä½",
        "alerts": [],
        "detailed_analysis": {}
    }
    
    if "Total Pss" not in stats:
        return alerts
    
    pss_stat = stats["Total Pss"]
    growth_rate = ((pss_stat['æœ€å¤§å€¼'] - pss_stat['æœ€å°å€¼']) / pss_stat['æœ€å°å€¼']) * 100
    
    # åˆ†æè¿ç»­å¢é•¿è¶‹åŠ¿
    pss_values = []
    for row in data_rows:
        try:
            pss_values.append(float(row["Total Pss"].replace(',', '')))
        except:
            continue
    
    # æ£€æµ‹è¿ç»­å¢é•¿æ®µ
    continuous_growth_count = 0
    max_continuous_growth = 0
    
    for i in range(1, len(pss_values)):
        if pss_values[i] > pss_values[i-1]:
            continuous_growth_count += 1
            max_continuous_growth = max(max_continuous_growth, continuous_growth_count)
        else:
            continuous_growth_count = 0
    
    # è®¡ç®—å³°å€¼ä¸å¹³å‡å€¼æ¯”ä¾‹
    peak_to_avg_ratio = pss_stat['æœ€å¤§å€¼'] / pss_stat['å¹³å‡å€¼']
    
    # åˆ†ææœ€è¿‘è¶‹åŠ¿ï¼ˆæœ€å20%çš„æ•°æ®ï¼‰
    recent_data_size = max(5, len(pss_values) // 5)
    recent_values = pss_values[-recent_data_size:]
    recent_growth = ((recent_values[-1] - recent_values[0]) / recent_values[0]) * 100 if len(recent_values) > 1 else 0
    
    alerts["detailed_analysis"] = {
        "æ€»ä½“å¢é•¿ç‡": f"{growth_rate:.1f}%",
        "æœ€å¤§è¿ç»­å¢é•¿å‘¨æœŸ": max_continuous_growth,
        "å³°å€¼å¹³å‡æ¯”": f"{peak_to_avg_ratio:.2f}",
        "è¿‘æœŸå¢é•¿ç‡": f"{recent_growth:.1f}%",
        "æ•°æ®ç‚¹æ•°": len(pss_values)
    }
    
    # ä¸¥é‡å‘Šè­¦æ¡ä»¶
    if (growth_rate > ALERT_THRESHOLDS["memory_leak"]["growth_rate_critical"] and 
        max_continuous_growth >= ALERT_THRESHOLDS["memory_leak"]["continuous_growth_periods"]):
        alerts["risk_level"] = "ä¸¥é‡"
        alerts["alerts"].append("ğŸš¨ ä¸¥é‡å†…å­˜æ³„æ¼é£é™©ï¼šå†…å­˜æŒç»­å¤§å¹…å¢é•¿ï¼Œå»ºè®®ç«‹å³æ£€æŸ¥")
        
    elif (growth_rate > ALERT_THRESHOLDS["memory_leak"]["growth_rate_critical"] or
          (recent_growth > 30 and max_continuous_growth >= 3)):
        alerts["risk_level"] = "é«˜"
        alerts["alerts"].append("âš ï¸ é«˜å†…å­˜æ³„æ¼é£é™©ï¼šæ£€æµ‹åˆ°æ˜æ˜¾çš„å†…å­˜å¢é•¿è¶‹åŠ¿")
        
    elif growth_rate > ALERT_THRESHOLDS["memory_leak"]["growth_rate_warning"]:
        alerts["risk_level"] = "ä¸­"
        alerts["alerts"].append("âš ï¸ ä¸­ç­‰å†…å­˜æ³„æ¼é£é™©ï¼šå†…å­˜ä½¿ç”¨é‡å¢é•¿è¾ƒå¿«")
    
    # é¢å¤–æ£€æŸ¥
    if peak_to_avg_ratio > ALERT_THRESHOLDS["memory_leak"]["peak_to_avg_ratio"]:
        alerts["alerts"].append(f"ğŸ“Š å†…å­˜å³°å€¼å¼‚å¸¸ï¼šå³°å€¼æ˜¯å¹³å‡å€¼çš„{peak_to_avg_ratio:.1f}å€")
    
    if recent_growth > 20:
        alerts["alerts"].append(f"ğŸ“ˆ è¿‘æœŸå†…å­˜å¢é•¿åŠ é€Ÿï¼šæœ€è¿‘å¢é•¿{recent_growth:.1f}%")
    
    return alerts

def analyze_performance_alerts(data_rows: List[Dict], stats: Dict) -> Dict[str, Any]:
    """ç»¼åˆæ€§èƒ½å‘Šè­¦åˆ†æ"""
    all_alerts = {
        "priority_alerts": [],
        "warning_alerts": [],
        "info_alerts": [],
        "overall_health": "è‰¯å¥½"
    }
    
    # å†…å­˜æ³„æ¼åˆ†æ
    memory_analysis = analyze_memory_leak_risk(data_rows, stats)
    if memory_analysis["risk_level"] in ["ä¸¥é‡", "é«˜"]:
        all_alerts["priority_alerts"].extend(memory_analysis["alerts"])
        all_alerts["overall_health"] = "ä¸¥é‡" if memory_analysis["risk_level"] == "ä¸¥é‡" else "è­¦å‘Š"
    elif memory_analysis["risk_level"] == "ä¸­":
        all_alerts["warning_alerts"].extend(memory_analysis["alerts"])
        if all_alerts["overall_health"] == "è‰¯å¥½":
            all_alerts["overall_health"] = "æ³¨æ„"
    
    # çº¿ç¨‹ç®¡ç†å‘Šè­¦
    if "Threads" in stats:
        thread_stat = stats["Threads"]
        max_threads = thread_stat['æœ€å¤§å€¼']
        thread_variance = thread_stat['æœ€å¤§å€¼'] - thread_stat['æœ€å°å€¼']
        
        if max_threads > ALERT_THRESHOLDS["thread_management"]["max_threads_critical"]:
            all_alerts["priority_alerts"].append(f"ğŸš¨ çº¿ç¨‹æ•°ä¸¥é‡è¶…æ ‡ï¼šå³°å€¼{max_threads}ï¼Œå»ºè®®ç«‹å³ä¼˜åŒ–")
            all_alerts["overall_health"] = "ä¸¥é‡"
        elif max_threads > ALERT_THRESHOLDS["thread_management"]["max_threads_warning"]:
            all_alerts["warning_alerts"].append(f"âš ï¸ çº¿ç¨‹æ•°åé«˜ï¼šå³°å€¼{max_threads}ï¼Œå»ºè®®ä¼˜åŒ–")
            if all_alerts["overall_health"] == "è‰¯å¥½":
                all_alerts["overall_health"] = "æ³¨æ„"
        
        if thread_variance > ALERT_THRESHOLDS["thread_management"]["thread_variance_critical"]:
            all_alerts["priority_alerts"].append(f"ğŸš¨ çº¿ç¨‹æ³¢åŠ¨ä¸¥é‡ï¼šå˜åŒ–èŒƒå›´{thread_variance}ï¼Œçº¿ç¨‹ç®¡ç†ä¸ç¨³å®š")
        elif thread_variance > ALERT_THRESHOLDS["thread_management"]["thread_variance_warning"]:
            all_alerts["warning_alerts"].append(f"âš ï¸ çº¿ç¨‹æ³¢åŠ¨è¾ƒå¤§ï¼šå˜åŒ–èŒƒå›´{thread_variance}")
    
    # å †å†…å­˜å‘Šè­¦
    heap_fields = ["Heap Size(J)", "Heap Size(N)"]
    for field in heap_fields:
        if field in stats:
            heap_stat = stats[field]
            heap_growth = ((heap_stat['æœ€å¤§å€¼'] - heap_stat['æœ€å°å€¼']) / heap_stat['æœ€å°å€¼']) * 100
            
            if heap_growth > ALERT_THRESHOLDS["heap_usage"]["heap_growth_critical"]:
                all_alerts["priority_alerts"].append(f"ğŸš¨ {field}ä¸¥é‡å¢é•¿ï¼šå¢é•¿{heap_growth:.1f}%")
            elif heap_growth > ALERT_THRESHOLDS["heap_usage"]["heap_growth_warning"]:
                all_alerts["warning_alerts"].append(f"âš ï¸ {field}å¢é•¿è¾ƒå¿«ï¼šå¢é•¿{heap_growth:.1f}%")
    
    # ç³»ç»Ÿèµ„æºå‘Šè­¦
    if "FD" in stats:
        fd_stat = stats["FD"]
        if fd_stat['æœ€å¤§å€¼'] > ALERT_THRESHOLDS["system_resources"]["fd_critical"]:
            all_alerts["priority_alerts"].append(f"ğŸš¨ æ–‡ä»¶æè¿°ç¬¦ä¸¥é‡è¶…æ ‡ï¼šå³°å€¼{fd_stat['æœ€å¤§å€¼']}ï¼Œå¯èƒ½å¯¼è‡´ç³»ç»Ÿä¸ç¨³å®š")
        elif fd_stat['æœ€å¤§å€¼'] > ALERT_THRESHOLDS["system_resources"]["fd_warning"]:
            all_alerts["warning_alerts"].append(f"âš ï¸ æ–‡ä»¶æè¿°ç¬¦åé«˜ï¼šå³°å€¼{fd_stat['æœ€å¤§å€¼']}ï¼Œå»ºè®®æ£€æŸ¥æ–‡ä»¶å¥æŸ„ç®¡ç†")
    
    if "Views" in stats:
        views_stat = stats["Views"]
        views_growth = ((views_stat['æœ€å¤§å€¼'] - views_stat['æœ€å°å€¼']) / views_stat['æœ€å°å€¼']) * 100
        if views_growth > ALERT_THRESHOLDS["system_resources"]["views_growth_critical"]:
            all_alerts["priority_alerts"].append(f"ğŸš¨ è§†å›¾æ•°é‡å¼‚å¸¸å¢é•¿ï¼šå¢é•¿{views_growth:.1f}%ï¼Œå¯èƒ½å­˜åœ¨è§†å›¾æ³„æ¼")
    
    # WebViewç‰¹æ®Šæ£€æŸ¥
    if "WebViews" in stats and stats["WebViews"]['æœ€å¤§å€¼'] > 0:
        webview_count = stats["WebViews"]['æœ€å¤§å€¼']
        if webview_count > 5:
            all_alerts["warning_alerts"].append(f"âš ï¸ WebViewæ•°é‡è¾ƒå¤šï¼š{webview_count}ä¸ªï¼Œæ³¨æ„å†…å­˜ç®¡ç†")
        else:
            all_alerts["info_alerts"].append(f"â„¹ï¸ æ£€æµ‹åˆ°WebViewä½¿ç”¨ï¼š{webview_count}ä¸ªï¼Œå»ºè®®ä¼˜åŒ–å†…å­˜ç®¡ç†")
    
    # æ·»åŠ å†…å­˜åˆ†æè¯¦æƒ…åˆ°info
    if memory_analysis["detailed_analysis"]:
        all_alerts["memory_analysis"] = memory_analysis["detailed_analysis"]
    
    return all_alerts

def parse_performance_data(csv_content: str) -> Dict[str, Any]:
    """è§£ææ€§èƒ½ç›‘æ§CSVæ•°æ®"""
    try:
        # ä½¿ç”¨StringIOæ¥å¤„ç†CSVå†…å®¹
        csv_file = io.StringIO(csv_content)
        reader = csv.DictReader(csv_file)
        
        data_rows = []
        for row in reader:
            data_rows.append(row)
        
        if not data_rows:
            return {"error": "æ²¡æœ‰æ‰¾åˆ°æ•°æ®è¡Œ"}
        
        # è·å–åˆ—å
        columns = list(data_rows[0].keys())
        
        # åˆ†ææ•°æ®
        analysis = {
            "æ€»è®°å½•æ•°": len(data_rows),
            "åˆ—æ•°": len(columns),
            "åˆ—å": columns,
            "æ—¶é—´èŒƒå›´": {
                "å¼€å§‹æ—¶é—´": data_rows[0].get("Time", "æœªçŸ¥"),
                "ç»“æŸæ—¶é—´": data_rows[-1].get("Time", "æœªçŸ¥")
            }
        }
        
        # åˆ†ææ•°å€¼å‹å­—æ®µçš„ç»Ÿè®¡ä¿¡æ¯
        numeric_stats = {}
        numeric_fields = [
            "Total Pss", "Heap Size(J)", "Heap Alloc(J)", "Heap Free(J)",
            "Heap Size(N)", "Heap Alloc(N)", "Heap Free(N)", "FD", "Views",
            "ViewRootImpl", "AppContexts", "Activities", "WebViews", "Thread",
            "HandlerThread", "Bitmap", "VmPeak", "VmSize", "VmHWM", "VmRSS", "Threads"
        ]
        
        for field in numeric_fields:
            if field in columns:
                values = []
                for row in data_rows:
                    try:
                        value = float(row[field].replace(',', ''))
                        values.append(value)
                    except (ValueError, AttributeError):
                        continue
                
                if values:
                    numeric_stats[field] = {
                        "æœ€å°å€¼": min(values),
                        "æœ€å¤§å€¼": max(values),
                        "å¹³å‡å€¼": round(sum(values) / len(values), 2),
                        "å˜åŒ–è¶‹åŠ¿": "ä¸Šå‡" if values[-1] > values[0] else "ä¸‹é™" if values[-1] < values[0] else "ç¨³å®š"
                    }
        
        analysis["æ•°å€¼ç»Ÿè®¡"] = numeric_stats
        
        # å†…å­˜ä½¿ç”¨åˆ†æ
        memory_analysis = {}
        if "Total Pss" in columns:
            pss_values = []
            for row in data_rows:
                try:
                    pss_values.append(float(row["Total Pss"].replace(',', '')))
                except:
                    continue
            
            if pss_values:
                memory_analysis["å†…å­˜ä½¿ç”¨"] = {
                    "åˆå§‹å€¼": f"{pss_values[0]:,.0f} KB",
                    "æœ€ç»ˆå€¼": f"{pss_values[-1]:,.0f} KB",
                    "å³°å€¼": f"{max(pss_values):,.0f} KB",
                    "å˜åŒ–": f"{((pss_values[-1] - pss_values[0]) / pss_values[0] * 100):+.1f}%"
                }
        
        # çº¿ç¨‹åˆ†æ
        if "Threads" in columns:
            thread_values = []
            for row in data_rows:
                try:
                    thread_values.append(int(row["Threads"]))
                except:
                    continue
            
            if thread_values:
                memory_analysis["çº¿ç¨‹æ•°"] = {
                    "åˆå§‹å€¼": thread_values[0],
                    "æœ€ç»ˆå€¼": thread_values[-1],
                    "å³°å€¼": max(thread_values),
                    "å˜åŒ–": thread_values[-1] - thread_values[0]
                }
        
        if memory_analysis:
            analysis["å…³é”®æŒ‡æ ‡åˆ†æ"] = memory_analysis
        
        # æ™ºèƒ½å‘Šè­¦åˆ†æ
        alert_analysis = analyze_performance_alerts(data_rows, numeric_stats)
        analysis["å‘Šè­¦åˆ†æ"] = alert_analysis
        
        # ä¼ ç»Ÿæ€§èƒ½å»ºè®®ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
        suggestions = []
        if "Total Pss" in numeric_stats:
            pss_trend = numeric_stats["Total Pss"]["å˜åŒ–è¶‹åŠ¿"]
            if pss_trend == "ä¸Šå‡":
                suggestions.append("å†…å­˜ä½¿ç”¨å‘ˆä¸Šå‡è¶‹åŠ¿ï¼Œå»ºè®®æ£€æŸ¥å†…å­˜æ³„æ¼")
        
        if "Threads" in numeric_stats:
            max_threads = numeric_stats["Threads"]["æœ€å¤§å€¼"]
            if max_threads > 300:
                suggestions.append(f"çº¿ç¨‹æ•°è¾ƒé«˜({max_threads})ï¼Œå»ºè®®ä¼˜åŒ–çº¿ç¨‹ç®¡ç†")
        
        if "WebViews" in numeric_stats:
            max_webviews = numeric_stats["WebViews"]["æœ€å¤§å€¼"]
            if max_webviews > 0:
                suggestions.append(f"æ£€æµ‹åˆ°WebViewä½¿ç”¨({max_webviews})ï¼Œæ³¨æ„WebViewå†…å­˜ç®¡ç†")
        
        if suggestions:
            analysis["æ€§èƒ½å»ºè®®"] = suggestions
        
        return analysis
        
    except Exception as e:
        return {"error": f"è§£æCSVæ•°æ®æ—¶å‡ºé”™: {str(e)}"}

def analyze_text_content(content: str, url: str, content_type: str) -> Dict[str, Any]:
    """åˆ†ææ–‡æœ¬å†…å®¹ï¼Œä¸“é—¨å¤„ç†æ€§èƒ½ç›‘æ§æ•°æ®"""
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯CSVæ ¼å¼çš„æ€§èƒ½æ•°æ®
    if "Time,Total Pss,Heap Size" in content or content.strip().startswith("Time,"):
        # è¿™æ˜¯æ€§èƒ½ç›‘æ§CSVæ•°æ®
        csv_analysis = parse_performance_data(content)
        
        return {
            "url": url,
            "content_type": content_type,
            "data_type": "performance_monitoring_csv",
            "is_text": True,
            "analysis": csv_analysis,
            "content_preview": content[:500] + "..." if len(content) > 500 else content
        }
    
    # å¦‚æœä¸æ˜¯æ€§èƒ½æ•°æ®ï¼Œè¿›è¡Œé€šç”¨æ–‡æœ¬åˆ†æ
    word_count = len(content.split())
    char_count = len(content)
    line_count = len(content.split('\n'))
    
    # Extract URLs if any
    url_pattern = r'https?://[^\s<>"{}|\\^`\[\]]+'
    urls_found = re.findall(url_pattern, content)
    
    # Extract email addresses
    email_pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
    emails_found = re.findall(email_pattern, content)
    
    # Simple keyword extraction (most common words)
    words = re.findall(r'\b\w+\b', content.lower())
    word_freq = {}
    for word in words:
        if len(word) > 3:  # Only count words longer than 3 characters
            word_freq[word] = word_freq.get(word, 0) + 1
    
    # Get top 10 most common words
    top_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:10]
    
    return {
        "url": url,
        "content_type": content_type,
        "data_type": "general_text",
        "is_text": True,
        "analysis": {
            "word_count": word_count,
            "character_count": char_count,
            "line_count": line_count,
            "urls_found": len(urls_found),
            "emails_found": len(emails_found),
            "top_words": top_words,
            "sample_urls": urls_found[:5] if urls_found else [],
            "sample_emails": emails_found[:5] if emails_found else []
        },
        "content_preview": content[:500] + "..." if len(content) > 500 else content
    }

if MCP_AVAILABLE:
    # åˆ›å»ºFastMCPæœåŠ¡å™¨å®ä¾‹
    mcp = FastMCP("Performance Data Analyzer")

    @mcp.tool()
    def fetch_and_analyze_url(url: str) -> Dict[str, Any]:
        """Fetch content from a URL and analyze it, with special handling for performance monitoring CSV data"""
        try:
            with urllib.request.urlopen(url) as response:
                content = response.read()
                content_type = response.info().get_content_type()
                
                # Try to decode as text
                try:
                    if content_type and ('text' in content_type or 'csv' in content_type):
                        text_content = content.decode('utf-8')
                        return analyze_text_content(text_content, url, content_type)
                    else:
                        return {
                            "url": url,
                            "content_type": content_type,
                            "size_bytes": len(content),
                            "analysis": "Binary content - cannot analyze text",
                            "is_text": False
                        }
                except UnicodeDecodeError:
                    return {
                        "url": url,
                        "content_type": content_type,
                        "size_bytes": len(content),
                        "analysis": "Content encoding not supported",
                        "is_text": False
                    }
                    
        except urllib.error.URLError as e:
            return {
                "url": url,
                "error": str(e),
                "analysis": "Failed to fetch URL"
            }

    if __name__ == "__main__":
        # è¿è¡ŒFastMCPæœåŠ¡å™¨
        mcp.run()
else:
    print("FastMCP not available. Please install with: pip install mcp")
    print("Or use the manual implementation in main.py") 