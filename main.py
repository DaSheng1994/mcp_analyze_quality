#!/usr/bin/env python3

import urllib.request
import urllib.error
import re
import csv
import io
from typing import Dict, Any, List
from datetime import datetime

# è¿™ä¸ªç‰ˆæœ¬éœ€è¦å®‰è£…: pip install mcp
try:
    from mcp.server.fastmcp import FastMCP
    MCP_AVAILABLE = True
except ImportError:
    MCP_AVAILABLE = False
    print("Warning: MCP library not installed. Run 'pip install mcp' to use FastMCP version.")

# å‘Šè­¦é˜ˆå€¼é…ç½®
ALERT_THRESHOLDS = {
    "memory_leak": {
        "growth_rate_critical": 100,  # å†…å­˜å¢é•¿ç‡è¶…è¿‡100%ä¸ºä¸¥é‡
        "growth_rate_warning": 50,    # å†…å­˜å¢é•¿ç‡è¶…è¿‡50%ä¸ºè­¦å‘Š
        "continuous_growth_periods": 5,  # è¿ç»­å¢é•¿å‘¨æœŸæ•°
        "peak_to_avg_ratio": 1.5,    # å³°å€¼ä¸å¹³å‡å€¼æ¯”ä¾‹
    },
    "thread_management": {
        "max_threads_critical": 400,  # çº¿ç¨‹æ•°è¶…è¿‡400ä¸ºä¸¥é‡
        "max_threads_warning": 350,   # çº¿ç¨‹æ•°è¶…è¿‡350ä¸ºè­¦å‘Š
        "thread_variance_critical": 100,  # çº¿ç¨‹æ³¢åŠ¨è¶…è¿‡100ä¸ºä¸¥é‡
        "thread_variance_warning": 50,    # çº¿ç¨‹æ³¢åŠ¨è¶…è¿‡50ä¸ºè­¦å‘Š
    },
    "heap_usage": {
        "heap_growth_critical": 200,  # å †å¢é•¿ç‡è¶…è¿‡200%ä¸ºä¸¥é‡
        "heap_growth_warning": 100,   # å †å¢é•¿ç‡è¶…è¿‡100%ä¸ºè­¦å‘Š
        "heap_utilization_critical": 0.9,  # å †åˆ©ç”¨ç‡è¶…è¿‡90%ä¸ºä¸¥é‡
        "heap_j_growth_mb_critical": 80,   # Javaå †å¢é•¿è¶…è¿‡80MBä¸ºä¸¥é‡
        "heap_n_growth_mb_critical": 500,  # Nativeå †å¢é•¿è¶…è¿‡500MBä¸ºä¸¥é‡
    },
    "system_resources": {
        "fd_critical": 800,           # æ–‡ä»¶æè¿°ç¬¦è¶…è¿‡800ä¸ºä¸¥é‡
        "fd_warning": 600,            # æ–‡ä»¶æè¿°ç¬¦è¶…è¿‡600ä¸ºè­¦å‘Š
        "fd_growth_critical": 100,    # æ–‡ä»¶æè¿°ç¬¦å¢é•¿è¶…è¿‡100ä¸ºä¸¥é‡
        "views_growth_critical": 200, # è§†å›¾å¢é•¿ç‡è¶…è¿‡200%ä¸ºä¸¥é‡
    }
}

def analyze_memory_leak_risk(data_rows: List[Dict], stats: Dict) -> Dict[str, Any]:
    """åˆ†æå†…å­˜æ³„æ¼é£é™©"""
    alerts = {
        "risk_level": "ä½",
        "alerts": [],
        "detailed_analysis": {}
    }
    
    if "Total Pss" not in stats:
        return alerts
    
    pss_stat = stats["Total Pss"]
    growth_rate = ((pss_stat['æœ€å¤§å€¼'] - pss_stat['æœ€å°å€¼']) / pss_stat['æœ€å°å€¼']) * 100
    
    # åˆ†æè¿ç»­å¢é•¿è¶‹åŠ¿
    pss_values = []
    for row in data_rows:
        try:
            pss_values.append(float(row["Total Pss"].replace(',', '')))
        except:
            continue
    
    # æ£€æµ‹è¿ç»­å¢é•¿æ®µ
    continuous_growth_count = 0
    max_continuous_growth = 0
    
    for i in range(1, len(pss_values)):
        if pss_values[i] > pss_values[i-1]:
            continuous_growth_count += 1
            max_continuous_growth = max(max_continuous_growth, continuous_growth_count)
        else:
            continuous_growth_count = 0
    
    # è®¡ç®—å³°å€¼ä¸å¹³å‡å€¼æ¯”ä¾‹
    peak_to_avg_ratio = pss_stat['æœ€å¤§å€¼'] / pss_stat['å¹³å‡å€¼']
    
    # åˆ†ææœ€è¿‘è¶‹åŠ¿ï¼ˆæœ€å20%çš„æ•°æ®ï¼‰
    recent_data_size = max(5, len(pss_values) // 5)
    recent_values = pss_values[-recent_data_size:]
    recent_growth = ((recent_values[-1] - recent_values[0]) / recent_values[0]) * 100 if len(recent_values) > 1 else 0
    
    alerts["detailed_analysis"] = {
        "æ€»ä½“å¢é•¿ç‡": f"{growth_rate:.1f}%",
        "æœ€å¤§è¿ç»­å¢é•¿å‘¨æœŸ": max_continuous_growth,
        "å³°å€¼å¹³å‡æ¯”": f"{peak_to_avg_ratio:.2f}",
        "è¿‘æœŸå¢é•¿ç‡": f"{recent_growth:.1f}%",
        "æ•°æ®ç‚¹æ•°": len(pss_values)
    }
    
    # ä¸¥é‡å‘Šè­¦æ¡ä»¶
    if (growth_rate > ALERT_THRESHOLDS["memory_leak"]["growth_rate_critical"] and 
        max_continuous_growth >= ALERT_THRESHOLDS["memory_leak"]["continuous_growth_periods"]):
        alerts["risk_level"] = "ä¸¥é‡"
        alerts["alerts"].append("ğŸš¨ ä¸¥é‡å†…å­˜æ³„æ¼é£é™©ï¼šå†…å­˜æŒç»­å¤§å¹…å¢é•¿ï¼Œå»ºè®®ç«‹å³æ£€æŸ¥")
        
    elif (growth_rate > ALERT_THRESHOLDS["memory_leak"]["growth_rate_critical"] or
          (recent_growth > 30 and max_continuous_growth >= 3)):
        alerts["risk_level"] = "é«˜"
        alerts["alerts"].append("âš ï¸ é«˜å†…å­˜æ³„æ¼é£é™©ï¼šæ£€æµ‹åˆ°æ˜æ˜¾çš„å†…å­˜å¢é•¿è¶‹åŠ¿")
        
    elif growth_rate > ALERT_THRESHOLDS["memory_leak"]["growth_rate_warning"]:
        alerts["risk_level"] = "ä¸­"
        alerts["alerts"].append("âš ï¸ ä¸­ç­‰å†…å­˜æ³„æ¼é£é™©ï¼šå†…å­˜ä½¿ç”¨é‡å¢é•¿è¾ƒå¿«")
    
    # é¢å¤–æ£€æŸ¥
    if peak_to_avg_ratio > ALERT_THRESHOLDS["memory_leak"]["peak_to_avg_ratio"]:
        alerts["alerts"].append(f"ğŸ“Š å†…å­˜å³°å€¼å¼‚å¸¸ï¼šå³°å€¼æ˜¯å¹³å‡å€¼çš„{peak_to_avg_ratio:.1f}å€")
    
    if recent_growth > 20:
        alerts["alerts"].append(f"ğŸ“ˆ è¿‘æœŸå†…å­˜å¢é•¿åŠ é€Ÿï¼šæœ€è¿‘å¢é•¿{recent_growth:.1f}%")
    
    return alerts

def analyze_performance_alerts(data_rows: List[Dict], stats: Dict) -> Dict[str, Any]:
    """ç»¼åˆæ€§èƒ½å‘Šè­¦åˆ†æ"""
    all_alerts = {
        "priority_alerts": [],
        "warning_alerts": [],
        "info_alerts": [],
        "overall_health": "è‰¯å¥½"
    }
    
    # å†…å­˜æ³„æ¼åˆ†æ
    memory_analysis = analyze_memory_leak_risk(data_rows, stats)
    if memory_analysis["risk_level"] in ["ä¸¥é‡", "é«˜"]:
        all_alerts["priority_alerts"].extend(memory_analysis["alerts"])
        all_alerts["overall_health"] = "ä¸¥é‡" if memory_analysis["risk_level"] == "ä¸¥é‡" else "è­¦å‘Š"
    elif memory_analysis["risk_level"] == "ä¸­":
        all_alerts["warning_alerts"].extend(memory_analysis["alerts"])
        if all_alerts["overall_health"] == "è‰¯å¥½":
            all_alerts["overall_health"] = "æ³¨æ„"
    
    # çº¿ç¨‹ç®¡ç†å‘Šè­¦
    if "Threads" in stats:
        thread_stat = stats["Threads"]
        max_threads = thread_stat['æœ€å¤§å€¼']
        thread_variance = thread_stat['æœ€å¤§å€¼'] - thread_stat['æœ€å°å€¼']
        
        if max_threads > ALERT_THRESHOLDS["thread_management"]["max_threads_critical"]:
            all_alerts["priority_alerts"].append(f"ğŸš¨ çº¿ç¨‹æ•°ä¸¥é‡è¶…æ ‡ï¼šå³°å€¼{max_threads}ï¼Œå»ºè®®ç«‹å³ä¼˜åŒ–")
            all_alerts["overall_health"] = "ä¸¥é‡"
        elif max_threads > ALERT_THRESHOLDS["thread_management"]["max_threads_warning"]:
            all_alerts["warning_alerts"].append(f"âš ï¸ çº¿ç¨‹æ•°åé«˜ï¼šå³°å€¼{max_threads}ï¼Œå»ºè®®ä¼˜åŒ–")
            if all_alerts["overall_health"] == "è‰¯å¥½":
                all_alerts["overall_health"] = "æ³¨æ„"
        
        if thread_variance > ALERT_THRESHOLDS["thread_management"]["thread_variance_critical"]:
            all_alerts["priority_alerts"].append(f"ğŸš¨ çº¿ç¨‹æ³¢åŠ¨ä¸¥é‡ï¼šå˜åŒ–èŒƒå›´{thread_variance}ï¼Œçº¿ç¨‹ç®¡ç†ä¸ç¨³å®š")
        elif thread_variance > ALERT_THRESHOLDS["thread_management"]["thread_variance_warning"]:
            all_alerts["warning_alerts"].append(f"âš ï¸ çº¿ç¨‹æ³¢åŠ¨è¾ƒå¤§ï¼šå˜åŒ–èŒƒå›´{thread_variance}")
    
    # å †å†…å­˜å‘Šè­¦ - å¢å¼ºç‰ˆæœ¬ï¼Œæ£€æŸ¥ç»å¯¹å¢é•¿é‡å’Œå›è½æƒ…å†µ
    def check_heap_fallback(data_rows: List[Dict], field: str) -> bool:
        """æ£€æŸ¥å †å†…å­˜æ˜¯å¦æœ‰å›è½"""
        values = []
        for row in data_rows:
            try:
                value = float(row[field].replace(',', ''))
                values.append(value)
            except:
                continue
        
        if len(values) < 10:  # æ•°æ®ç‚¹å¤ªå°‘ï¼Œæ— æ³•åˆ¤æ–­
            return False
            
        # æ‰¾åˆ°å³°å€¼ä½ç½®
        max_value = max(values)
        max_index = values.index(max_value)
        
        # æ£€æŸ¥å³°å€¼åæ˜¯å¦æœ‰æ˜æ˜¾å›è½ï¼ˆè‡³å°‘å›è½20%ï¼‰
        if max_index < len(values) - 5:  # å³°å€¼åè‡³å°‘æœ‰5ä¸ªæ•°æ®ç‚¹
            post_peak_values = values[max_index:]
            min_post_peak = min(post_peak_values)
            fallback_ratio = (max_value - min_post_peak) / max_value
            return fallback_ratio > 0.2  # å›è½è¶…è¿‡20%è®¤ä¸ºæœ‰å›è½
        
        return False
    
    heap_fields = ["Heap Size(J)", "Heap Size(N)"]
    for field in heap_fields:
        if field in stats:
            heap_stat = stats[field]
            heap_growth_mb = (heap_stat['æœ€å¤§å€¼'] - heap_stat['æœ€å°å€¼']) / 1024  # è½¬æ¢ä¸ºMB
            heap_growth_rate = ((heap_stat['æœ€å¤§å€¼'] - heap_stat['æœ€å°å€¼']) / heap_stat['æœ€å°å€¼']) * 100
            
            # Javaå †å†…å­˜æ£€æŸ¥ï¼šå¢é•¿è¶…è¿‡80MB
            if field == "Heap Size(J)" and heap_growth_mb > ALERT_THRESHOLDS["heap_usage"]["heap_j_growth_mb_critical"]:
                all_alerts["priority_alerts"].append(f"ğŸš¨ Javaå †å†…å­˜å¢é•¿ä¸¥é‡è¶…æ ‡ï¼šå¢é•¿{heap_growth_mb:.1f}MBï¼Œå»ºè®®æ£€æŸ¥å†…å­˜æ³„æ¼")
                all_alerts["overall_health"] = "ä¸¥é‡"
            
            # Nativeå †å†…å­˜æ£€æŸ¥ï¼šå¢é•¿è¶…è¿‡500MBä¸”æ²¡æœ‰å›è½
            elif field == "Heap Size(N)" and heap_growth_mb > ALERT_THRESHOLDS["heap_usage"]["heap_n_growth_mb_critical"]:
                has_fallback = check_heap_fallback(data_rows, field)
                if not has_fallback:
                    all_alerts["priority_alerts"].append(f"ğŸš¨ Nativeå †å†…å­˜ä¸¥é‡å¢é•¿ä¸”æ— å›è½ï¼šå¢é•¿{heap_growth_mb:.1f}MBï¼Œå¯èƒ½å­˜åœ¨ä¸¥é‡å†…å­˜æ³„æ¼")
                    all_alerts["overall_health"] = "ä¸¥é‡"
                else:
                    all_alerts["warning_alerts"].append(f"âš ï¸ Nativeå †å†…å­˜å¢é•¿è¾ƒå¤§ä½†æœ‰å›è½ï¼šå¢é•¿{heap_growth_mb:.1f}MB")
            
            # ä¿ç•™åŸæœ‰çš„ç™¾åˆ†æ¯”æ£€æŸ¥ä½œä¸ºè¡¥å……
            elif heap_growth_rate > ALERT_THRESHOLDS["heap_usage"]["heap_growth_critical"]:
                all_alerts["priority_alerts"].append(f"ğŸš¨ {field}ä¸¥é‡å¢é•¿ï¼šå¢é•¿{heap_growth_rate:.1f}%")
            elif heap_growth_rate > ALERT_THRESHOLDS["heap_usage"]["heap_growth_warning"]:
                all_alerts["warning_alerts"].append(f"âš ï¸ {field}å¢é•¿è¾ƒå¿«ï¼šå¢é•¿{heap_growth_rate:.1f}%")
    
    # ç³»ç»Ÿèµ„æºå‘Šè­¦
    if "FD" in stats:
        fd_stat = stats["FD"]
        max_fd = fd_stat['æœ€å¤§å€¼']
        min_fd = fd_stat['æœ€å°å€¼']
        fd_growth = max_fd - min_fd  # ç»å¯¹å¢é•¿æ•°é‡
        
        # æ£€æŸ¥æ–‡ä»¶æè¿°ç¬¦å¢é•¿æ˜¯å¦è¶…è¿‡100ï¼ˆä¸¥é‡å‘Šè­¦ï¼‰
        if fd_growth > ALERT_THRESHOLDS["system_resources"]["fd_growth_critical"]:
            all_alerts["priority_alerts"].append(f"ğŸš¨ æ–‡ä»¶æè¿°ç¬¦å¢é•¿ä¸¥é‡è¶…æ ‡ï¼šå¢é•¿{fd_growth:.0f}ä¸ªï¼Œå¯èƒ½å­˜åœ¨èµ„æºæ³„æ¼")
            all_alerts["overall_health"] = "ä¸¥é‡"
        # æ£€æŸ¥æ–‡ä»¶æè¿°ç¬¦æ€»æ•°æ˜¯å¦è¶…æ ‡
        elif max_fd > ALERT_THRESHOLDS["system_resources"]["fd_critical"]:
            all_alerts["priority_alerts"].append(f"ğŸš¨ æ–‡ä»¶æè¿°ç¬¦ä¸¥é‡è¶…æ ‡ï¼šå³°å€¼{max_fd}ï¼Œå¯èƒ½å¯¼è‡´ç³»ç»Ÿä¸ç¨³å®š")
            all_alerts["overall_health"] = "ä¸¥é‡"
        elif max_fd > ALERT_THRESHOLDS["system_resources"]["fd_warning"]:
            all_alerts["warning_alerts"].append(f"âš ï¸ æ–‡ä»¶æè¿°ç¬¦åé«˜ï¼šå³°å€¼{max_fd}ï¼Œå»ºè®®æ£€æŸ¥æ–‡ä»¶å¥æŸ„ç®¡ç†")
    
    if "Views" in stats:
        views_stat = stats["Views"]
        max_views = views_stat['æœ€å¤§å€¼']
        min_views = views_stat['æœ€å°å€¼']
        views_growth_count = max_views - min_views  # ç»å¯¹å¢é•¿æ•°é‡
        views_growth_rate = ((max_views - min_views) / min_views) * 100  # å¢é•¿ç‡
        
        # æ£€æŸ¥è§†å›¾å¢é•¿æ•°é‡æ˜¯å¦è¶…è¿‡700ï¼ˆä¸¥é‡å‘Šè­¦ï¼‰
        if views_growth_count > 700:
            all_alerts["priority_alerts"].append(f"ğŸš¨ è§†å›¾æ•°é‡å¢é•¿ä¸¥é‡è¶…æ ‡ï¼šå¢é•¿{views_growth_count:.0f}ä¸ªï¼Œå¯èƒ½å¯¼è‡´å†…å­˜æº¢å‡º")
            all_alerts["overall_health"] = "ä¸¥é‡"
        # æ£€æŸ¥è§†å›¾å¢é•¿ç‡æ˜¯å¦å¼‚å¸¸
        elif views_growth_rate > ALERT_THRESHOLDS["system_resources"]["views_growth_critical"]:
            all_alerts["priority_alerts"].append(f"ğŸš¨ è§†å›¾æ•°é‡å¼‚å¸¸å¢é•¿ï¼šå¢é•¿{views_growth_rate:.1f}%ï¼Œå¯èƒ½å­˜åœ¨è§†å›¾æ³„æ¼")
    
    # WebViewç‰¹æ®Šæ£€æŸ¥
    if "WebViews" in stats and stats["WebViews"]['æœ€å¤§å€¼'] > 0:
        webview_count = stats["WebViews"]['æœ€å¤§å€¼']
        if webview_count > 5:
            all_alerts["warning_alerts"].append(f"âš ï¸ WebViewæ•°é‡è¾ƒå¤šï¼š{webview_count}ä¸ªï¼Œæ³¨æ„å†…å­˜ç®¡ç†")
        else:
            all_alerts["info_alerts"].append(f"â„¹ï¸ æ£€æµ‹åˆ°WebViewä½¿ç”¨ï¼š{webview_count}ä¸ªï¼Œå»ºè®®ä¼˜åŒ–å†…å­˜ç®¡ç†")
    
    # æ·»åŠ å†…å­˜åˆ†æè¯¦æƒ…åˆ°info
    if memory_analysis["detailed_analysis"]:
        all_alerts["memory_analysis"] = memory_analysis["detailed_analysis"]
    
    return all_alerts

def parse_performance_data(csv_content: str) -> Dict[str, Any]:
    """è§£ææ€§èƒ½ç›‘æ§CSVæ•°æ®"""
    try:
        # ä½¿ç”¨StringIOæ¥å¤„ç†CSVå†…å®¹
        csv_file = io.StringIO(csv_content)
        reader = csv.DictReader(csv_file)
        
        data_rows = []
        for row in reader:
            data_rows.append(row)
        
        if not data_rows:
            return {"error": "æ²¡æœ‰æ‰¾åˆ°æ•°æ®è¡Œ"}
        
        # è·å–åˆ—å
        columns = list(data_rows[0].keys())
        
        # åˆ†ææ•°æ®
        analysis = {
            "æ€»è®°å½•æ•°": len(data_rows),
            "åˆ—æ•°": len(columns),
            "åˆ—å": columns,
            "æ—¶é—´èŒƒå›´": {
                "å¼€å§‹æ—¶é—´": data_rows[0].get("Time", "æœªçŸ¥"),
                "ç»“æŸæ—¶é—´": data_rows[-1].get("Time", "æœªçŸ¥")
            }
        }
        
        # åˆ†ææ•°å€¼å‹å­—æ®µçš„ç»Ÿè®¡ä¿¡æ¯
        numeric_stats = {}
        numeric_fields = [
            "Total Pss", "Heap Size(J)", "Heap Alloc(J)", "Heap Free(J)",
            "Heap Size(N)", "Heap Alloc(N)", "Heap Free(N)", "FD", "Views",
            "ViewRootImpl", "AppContexts", "Activities", "WebViews", "Thread",
            "HandlerThread", "Bitmap", "VmPeak", "VmSize", "VmHWM", "VmRSS", "Threads"
        ]
        
        for field in numeric_fields:
            if field in columns:
                values = []
                for row in data_rows:
                    try:
                        value = float(row[field].replace(',', ''))
                        values.append(value)
                    except (ValueError, AttributeError):
                        continue
                
                if values:
                    numeric_stats[field] = {
                        "æœ€å°å€¼": min(values),
                        "æœ€å¤§å€¼": max(values),
                        "å¹³å‡å€¼": round(sum(values) / len(values), 2),
                        "å˜åŒ–è¶‹åŠ¿": "ä¸Šå‡" if values[-1] > values[0] else "ä¸‹é™" if values[-1] < values[0] else "ç¨³å®š"
                    }
        
        analysis["æ•°å€¼ç»Ÿè®¡"] = numeric_stats
        
        # å†…å­˜ä½¿ç”¨åˆ†æ
        memory_analysis = {}
        if "Total Pss" in columns:
            pss_values = []
            for row in data_rows:
                try:
                    pss_values.append(float(row["Total Pss"].replace(',', '')))
                except:
                    continue
            
            if pss_values:
                memory_analysis["å†…å­˜ä½¿ç”¨"] = {
                    "åˆå§‹å€¼": f"{pss_values[0]:,.0f} KB",
                    "æœ€ç»ˆå€¼": f"{pss_values[-1]:,.0f} KB",
                    "å³°å€¼": f"{max(pss_values):,.0f} KB",
                    "å˜åŒ–": f"{((pss_values[-1] - pss_values[0]) / pss_values[0] * 100):+.1f}%"
                }
        
        # çº¿ç¨‹åˆ†æ
        if "Threads" in columns:
            thread_values = []
            for row in data_rows:
                try:
                    thread_values.append(int(row["Threads"]))
                except:
                    continue
            
            if thread_values:
                memory_analysis["çº¿ç¨‹æ•°"] = {
                    "åˆå§‹å€¼": thread_values[0],
                    "æœ€ç»ˆå€¼": thread_values[-1],
                    "å³°å€¼": max(thread_values),
                    "å˜åŒ–": thread_values[-1] - thread_values[0]
                }
        
        if memory_analysis:
            analysis["å…³é”®æŒ‡æ ‡åˆ†æ"] = memory_analysis
        
        # æ™ºèƒ½å‘Šè­¦åˆ†æ
        alert_analysis = analyze_performance_alerts(data_rows, numeric_stats)
        analysis["å‘Šè­¦åˆ†æ"] = alert_analysis
        
        # ä¼ ç»Ÿæ€§èƒ½å»ºè®®ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
        suggestions = []
        if "Total Pss" in numeric_stats:
            pss_trend = numeric_stats["Total Pss"]["å˜åŒ–è¶‹åŠ¿"]
            if pss_trend == "ä¸Šå‡":
                suggestions.append("å†…å­˜ä½¿ç”¨å‘ˆä¸Šå‡è¶‹åŠ¿ï¼Œå»ºè®®æ£€æŸ¥å†…å­˜æ³„æ¼")
        
        if "Threads" in numeric_stats:
            max_threads = numeric_stats["Threads"]["æœ€å¤§å€¼"]
            if max_threads > 300:
                suggestions.append(f"çº¿ç¨‹æ•°è¾ƒé«˜({max_threads})ï¼Œå»ºè®®ä¼˜åŒ–çº¿ç¨‹ç®¡ç†")
        
        if "WebViews" in numeric_stats:
            max_webviews = numeric_stats["WebViews"]["æœ€å¤§å€¼"]
            if max_webviews > 0:
                suggestions.append(f"æ£€æµ‹åˆ°WebViewä½¿ç”¨({max_webviews})ï¼Œæ³¨æ„WebViewå†…å­˜ç®¡ç†")
        
        if suggestions:
            analysis["æ€§èƒ½å»ºè®®"] = suggestions
        
        return analysis
        
    except Exception as e:
        return {"error": f"è§£æCSVæ•°æ®æ—¶å‡ºé”™: {str(e)}"}

def analyze_text_content(content: str, url: str, content_type: str) -> Dict[str, Any]:
    """åˆ†ææ–‡æœ¬å†…å®¹ï¼Œä¸“é—¨å¤„ç†æ€§èƒ½ç›‘æ§æ•°æ® - ç®€åŒ–ç‰ˆæœ¬ï¼Œåªè¿”å›å…³é”®ç»“æœ"""
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯CSVæ ¼å¼çš„æ€§èƒ½æ•°æ®
    if "Time,Total Pss,Heap Size" in content or content.strip().startswith("Time,"):
        # è¿™æ˜¯æ€§èƒ½ç›‘æ§CSVæ•°æ®
        csv_analysis = parse_performance_data(content)
        
        # ç®€åŒ–åˆ†æç»“æœï¼Œåªä¿ç•™å…³é”®ä¿¡æ¯
        simplified_analysis = {}
        
        # åŸºæœ¬ä¿¡æ¯
        simplified_analysis["æ•°æ®æ¦‚è§ˆ"] = {
            "æ€»è®°å½•æ•°": csv_analysis.get("æ€»è®°å½•æ•°", 0),
            "ç›‘æ§æ—¶é•¿": csv_analysis.get("æ—¶é—´èŒƒå›´", {})
        }
        
        # åªä¿ç•™å…³é”®æŒ‡æ ‡
        if "å…³é”®æŒ‡æ ‡åˆ†æ" in csv_analysis:
            simplified_analysis["å…³é”®æŒ‡æ ‡"] = csv_analysis["å…³é”®æŒ‡æ ‡åˆ†æ"]
        
        # å‘Šè­¦åˆ†æ - é‡ç‚¹çªå‡º
        if "å‘Šè­¦åˆ†æ" in csv_analysis:
            alert_analysis = csv_analysis["å‘Šè­¦åˆ†æ"]
            simplified_analysis["å‘Šè­¦åˆ†æ"] = {
                "ç³»ç»Ÿå¥åº·çŠ¶æ€": alert_analysis.get("overall_health", "è‰¯å¥½"),
                "ä¸¥é‡å‘Šè­¦": alert_analysis.get("priority_alerts", []),
                "è­¦å‘Šå‘Šè­¦": alert_analysis.get("warning_alerts", [])
            }
            
            # åªåœ¨æœ‰ä¸¥é‡å†…å­˜æ³„æ¼æ—¶æ˜¾ç¤ºè¯¦ç»†å†…å­˜åˆ†æ
            if alert_analysis.get("overall_health") == "ä¸¥é‡" and "memory_analysis" in alert_analysis:
                memory_details = alert_analysis["memory_analysis"]
                simplified_analysis["å†…å­˜æ³„æ¼åˆ†æ"] = {
                    "æ€»ä½“å¢é•¿ç‡": memory_details.get("æ€»ä½“å¢é•¿ç‡"),
                    "è¿ç»­å¢é•¿å‘¨æœŸ": memory_details.get("æœ€å¤§è¿ç»­å¢é•¿å‘¨æœŸ"),
                    "é£é™©ç­‰çº§": "ğŸ”´ æé«˜" if float(memory_details.get("æ€»ä½“å¢é•¿ç‡", "0%").replace("%", "")) > 100 else "ğŸŸ¡ ä¸­ç­‰"
                }
        
        return {
            "url": url,
            "content_type": content_type,
            "data_type": "performance_monitoring_csv",
            "analysis": simplified_analysis
        }
    
    # å¦‚æœä¸æ˜¯æ€§èƒ½æ•°æ®ï¼Œè¿›è¡Œé€šç”¨æ–‡æœ¬åˆ†æï¼ˆä¿æŒç®€å•ï¼‰
    return {
        "url": url,
        "content_type": content_type,
        "data_type": "general_text",
        "analysis": {
            "word_count": len(content.split()),
            "line_count": len(content.split('\n')),
            "content_type": "éæ€§èƒ½ç›‘æ§æ•°æ®"
        }
    }

if MCP_AVAILABLE:
    # åˆ›å»ºFastMCPæœåŠ¡å™¨å®ä¾‹
    mcp = FastMCP("Performance Data Analyzer")

    @mcp.tool()
    def fetch_and_analyze_url(url: str) -> Dict[str, Any]:
        """Fetch content from a URL and analyze it, with special handling for performance monitoring CSV data"""
        try:
            with urllib.request.urlopen(url) as response:
                content = response.read()
                content_type = response.info().get_content_type()
                
                # Try to decode as text
                try:
                    if content_type and ('text' in content_type or 'csv' in content_type):
                        text_content = content.decode('utf-8')
                        return analyze_text_content(text_content, url, content_type)
                    else:
                        return {
                            "url": url,
                            "content_type": content_type,
                            "size_bytes": len(content),
                            "analysis": "Binary content - cannot analyze text",
                            "is_text": False
                        }
                except UnicodeDecodeError:
                    return {
                        "url": url,
                        "content_type": content_type,
                        "size_bytes": len(content),
                        "analysis": "Content encoding not supported",
                        "is_text": False
                    }
                    
        except urllib.error.URLError as e:
            return {
                "url": url,
                "error": str(e),
                "analysis": "Failed to fetch URL"
            }

    if __name__ == "__main__":
        import sys
        import os
        
        print(f"ğŸš€ å¯åŠ¨MCPæ€§èƒ½åˆ†ææœåŠ¡å™¨...")
        print(f"ğŸ“Š æ”¯æŒåŠŸèƒ½ï¼šå†…å­˜æ³„æ¼æ£€æµ‹ã€æ€§èƒ½å‘Šè­¦åˆ†æ")
        
        # è¿è¡ŒFastMCPæœåŠ¡å™¨ - ä½¿ç”¨stdioæ¨¡å¼
        mcp.run()
else:
    print("FastMCP not available. Please install with: pip install mcp")
    print("Or use the manual implementation in main.py") 